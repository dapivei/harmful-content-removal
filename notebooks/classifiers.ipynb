{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I89peoPpnKXh",
    "outputId": "f008a78b-ecbb-4234-8e97-af5f239200dc"
   },
   "outputs": [],
   "source": [
    "#!pip install ftfy regex tqdm\n",
    "#!pip install git+https://github.com/openai/CLIP.git\n",
<<<<<<< HEAD
    "#https://github.com/ml-research/Q16\n",
    "#!pip install --upgrade \"nudenet>=3.4.2\"\n",
=======
    "#!pip install --upgrade \"nudenet>=3.4.2\"\n",
    "\n",
    "#https://github.com/ml-research/Q16\n",
>>>>>>> 7525fd3fa0629337ae481b0065f628f3cc42bc03
    "#https://github.com/notAI-tech/NudeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXjItVSDl-pd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import PIL\n",
    "import pickle\n",
    "import clip\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from nudenet import NudeDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYyYFjQQl_Fi"
   },
   "outputs": [],
   "source": [
    "class ClipWrapper(torch.nn.Module):\n",
    "    def __init__(self, device, model_name='ViT-L/14'):\n",
    "        super(ClipWrapper, self).__init__()\n",
    "        self.clip_model, self.preprocess = clip.load(\n",
    "            model_name,\n",
    "            device,\n",
    "            jit=False\n",
    "        )\n",
    "        self.clip_model.eval()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.clip_model.encode_image(x)\n",
    "\n",
    "\n",
    "class SimClassifier(torch.nn.Module):\n",
    "    def __init__(self, embeddings, device):\n",
    "        super(SimClassifier, self).__init__()\n",
    "        self.embeddings = torch.nn.parameter.Parameter(embeddings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeddings_norm = self.embeddings / self.embeddings.norm(dim=-1,\n",
    "                                                                 keepdim=True)\n",
    "        # Pick the top 5 most similar labels for the image\n",
    "        image_features_norm = x / x.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (100.0 * image_features_norm @ embeddings_norm.T)\n",
    "        # values, indices = similarity[0].topk(5)\n",
    "        return similarity.squeeze()\n",
    "\n",
    "def initialize_prompts(clip_model, text_prompts, device):\n",
    "    text = clip.tokenize(text_prompts).to(device)\n",
    "    return clip_model.encode_text(text)\n",
    "\n",
    "\n",
    "def save_prompts(classifier, save_path):\n",
    "    prompts = classifier.embeddings.detach().cpu().numpy()\n",
    "    pickle.dump(prompts, open(save_path, 'wb'))\n",
    "\n",
    "\n",
    "def load_prompts(file_path, device):\n",
    "    return torch.HalfTensor(pickle.load(open(file_path, 'rb'))).to(device)\n",
    "\n",
    "def compute_embeddings(image_paths):\n",
    "    images = [clip.preprocess(PIL.Image.open(image_path)) for image_path in image_paths]\n",
    "    images = torch.stack(images).to(device)\n",
    "    return clip(images).half()\n",
    "\n",
    "def classify_images_batches(image_files, batch_size=30):\n",
    "    results = []\n",
    "    detector = NudeDetector()\n",
    "    for i in tqdm(range(0, len(image_files), batch_size), desc=\"Processing batches\"):\n",
    "        batch = image_files[i:i + batch_size]  # Get the current batch\n",
    "        batch_embeddings = compute_embeddings(batch)  # Process the batch\n",
    "        batch_embeddings = batch_embeddings.to(device)\n",
    "        nudes = detector.detect_batch(batch)\n",
    "        y = classifier(batch_embeddings)\n",
    "        y = torch.argmax(y, dim=1)  # Get the predicted labels\n",
    "        for file, q16, nude in zip(batch, y.tolist(), nudes):\n",
    "            results.append({'file': file, 'q16': q16, 'nude': nude})\n",
    "        torch.cuda.empty_cache()\n",
    "    df = pd.DataFrame(results)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nnAGIMaOl_Yw",
    "outputId": "99a9d569-16b8-4716-be4a-37084e8dd25f"
   },
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "prompt_path = '../data/classifiers/prompts.p'\n",
<<<<<<< HEAD
    "trained_prompts = load_prompts(prompt_path, device=device)"
=======
    "trained_prompts = load_prompts(\n",
    "    prompt_path, device=device\n",
    ")"
>>>>>>> 7525fd3fa0629337ae481b0065f628f3cc42bc03
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0ajWeFqmAex",
    "outputId": "5da2636a-9c79-4994-e5d1-51543a5a4b49"
   },
   "outputs": [],
   "source": [
    "clip = ClipWrapper(device)\n",
    "print('initialized clip model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKfbXJglmAyc",
    "outputId": "c5d543df-340d-426c-87a9-11266f011eea"
   },
   "outputs": [],
   "source": [
    "classifier = SimClassifier(trained_prompts, device)\n",
    "print('initialized classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base = \"../data/images/baseline/\"\n",
    "image_files = [\n",
<<<<<<< HEAD
    "    os.path.join(image_base, file) for file in os.listdir(image_base) if file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n",
    "]\n",
    "base = classify_images_batches(\n",
    "    image_files, batch_size=10\n",
    ")\n",
    "\n",
    "base.to_csv(\"../data/classifiers/labels_base.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_contr = \"../data/images/control/\"\n",
    "image_files = [\n",
    "    os.path.join(image_contr, file) for file in os.listdir(image_contr) if file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n",
    "]\n",
    "con = classify_images_batches(\n",
    "    image_files, batch_size=10\n",
    ")\n",
    "\n",
    "con.to_csv(\"../data/classifiers/labels_contr.csv\")"
=======
    "    os.path.join(image_dir, file) \n",
    "    for file in os.listdir(image_dir) if file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))\n",
    "]"
>>>>>>> 7525fd3fa0629337ae481b0065f628f3cc42bc03
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = classify_images_batches(\n",
    "    image_files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/classifiers/images_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "harmful",
   "language": "python",
   "name": "harmful_environ"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
