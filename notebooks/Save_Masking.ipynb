{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29d6f5ce-553b-4803-b54e-1c3d395b47f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "from IPython.display import display, HTML\n",
    "from torchvision import transforms \n",
    "import cv2\n",
    "import pickle\n",
    "import os\n",
    "# Import utility functions from util.py\n",
    "from util import (\n",
    "    show_anns_on_image,\n",
    "    batchify,\n",
    "    combine_harmful_masks,\n",
    "    resize_image,\n",
    "    mask_harmful_content,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b594b1ec-5a9c-4503-b14f-7e55527b367e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77599c39-941c-48ce-a6ce-4ffd8654a28f",
   "metadata": {},
   "source": [
    "## Uncomment if you need to download SAM's weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "974b1991-e2d6-4985-beeb-d93d4564d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # URL to the weight file\n",
    "# url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "\n",
    "# # Path where you want to save the file\n",
    "# output_path = \"sam_vit_h_4b8939.pth\"\n",
    "\n",
    "# # Download the file\n",
    "# print(\"Downloading SAM model weights...\")\n",
    "# response = requests.get(url, stream=True)\n",
    "# if response.status_code == 200:\n",
    "#     with open(output_path, \"wb\") as f:\n",
    "#         for chunk in response.iter_content(chunk_size=1024):\n",
    "#             f.write(chunk)\n",
    "#     print(f\"Downloaded SAM model weights to {output_path}\")\n",
    "# else:\n",
    "#     print(f\"Failed to download the weights. HTTP status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b02e3-2630-4798-8f9d-be8a1b9d1fbf",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29808bc-c780-4595-9df3-2a824895d94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yc7087/.local/lib/python3.9/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "# Global variables for models\n",
    "# Load SAM model\n",
    "model_type = \"vit_h\"  # Options: 'vit_h', 'vit_l', 'vit_b'\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "# Load CLIP model\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d75c97-0287-43fd-b189-2b939c19ec03",
   "metadata": {},
   "source": [
    "## Util functions: segment image and classify segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b11c36e9-abb2-4060-828e-7c7da65ba10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(image):\n",
    "    \"\"\"\n",
    "    Segments the image using SAM (Segment Anything Model).\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Image to segment.\n",
    "\n",
    "    Returns:\n",
    "        list: List of segmentation masks.\n",
    "    \"\"\"\n",
    "    # Convert PIL image to NumPy array\n",
    "    image_np = np.array(image)\n",
    "    image_np = cv2.resize(image_np, (1024, 1024), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    mask_generator = SamAutomaticMaskGenerator(\n",
    "        sam,\n",
    "        points_per_side=32,           # Adjust for finer or coarser grid, 64\n",
    "        min_mask_region_area=50,      # Set minimum area for masks\n",
    "        box_nms_thresh=0.2,           # Adjust NMS threshold\n",
    "        stability_score_thresh=0.2,   # Set stability score threshold\n",
    "    )\n",
    "\n",
    "    mask_generator.predictor.model.to(device)\n",
    "\n",
    "    # image_tensor = torch.tensor(image_np).to(device)\n",
    "    masks = mask_generator.generate(image_np)\n",
    "\n",
    "    print(f\"Generated {len(masks)} masks.\")\n",
    "    return masks\n",
    "\n",
    "def classify_segments(image, masks, descriptions):\n",
    "    \"\"\"\n",
    "    Classifies each image segment using CLIP.\n",
    "\n",
    "    Args:\n",
    "        image (PIL.Image.Image): Original image.\n",
    "        masks (list): List of segmentation masks.\n",
    "        descriptions (list): List of descriptions for classification.\n",
    "\n",
    "    Returns:\n",
    "        tuple: overall_probs, overall_masks\n",
    "    \"\"\"\n",
    "    # Preprocess segments\n",
    "    res = []\n",
    "    for mask in masks:\n",
    "        segmentation = mask['segmentation']\n",
    "\n",
    "        # Mask out all other parts\n",
    "        segment_image = np.array(image).copy()\n",
    "        mask_bool = segmentation.astype(bool)\n",
    "        segment_image[~mask_bool] = 255  # Set the background to white\n",
    "\n",
    "        # Resize the image to 224x224\n",
    "        segment_resized = resize_image(segment_image)\n",
    "        res.append(segment_resized)\n",
    "\n",
    "    # Define batch size\n",
    "    batch_size = 16\n",
    "\n",
    "    # Split images into batches\n",
    "    image_batches = batchify(res, batch_size)\n",
    "\n",
    "    # Initialize lists to store probabilities and masks\n",
    "    overall_probs = []\n",
    "    overall_masks = []\n",
    "\n",
    "    # Process each batch\n",
    "    for batch_idx, image_batch in enumerate(image_batches):\n",
    "        # Process inputs in a batch\n",
    "        inputs = clip_processor(\n",
    "            text=descriptions,\n",
    "            images=image_batch,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Move inputs to the appropriate device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Perform inference in a batch\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "        \n",
    "        # Extract logits and compute probabilities\n",
    "        logits_per_image = outputs.logits_per_image  # Shape: [batch_size, num_descriptions]\n",
    "        probs = logits_per_image.softmax(dim=1)      # Shape: [batch_size, num_descriptions]\n",
    "        \n",
    "        # Process results for each image in the batch\n",
    "        for sub_batch_idx, text_probs in enumerate(probs):\n",
    "            global_image_idx = batch_idx * batch_size + sub_batch_idx  # Absolute image index\n",
    "            overall_probs.append(text_probs.cpu().numpy())\n",
    "            overall_masks.append(masks[global_image_idx])\n",
    "            descs = list(descriptions)  # List of descriptions\n",
    "            text_probs = text_probs.cpu().numpy() * 100  # Convert to percentages\n",
    "            \n",
    "            # Find the index of the maximum probability\n",
    "            max_index = text_probs.argmax()\n",
    "            \n",
    "            # print(f\"Segment {global_image_idx + 1}:\")\n",
    "            # for i, (desc, prob) in enumerate(zip(descs, text_probs)):\n",
    "            #     if i == max_index:\n",
    "            #         display(HTML(f\"<span style='color: red;'>{desc}: {prob:.2f}%</span>\"))\n",
    "            #     else:\n",
    "            #         print(f\"{desc}: {prob:.2f}%\")\n",
    "            # print()\n",
    "    return overall_probs, overall_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7a8d1-6ff3-4e28-ad72-9a042c5c13f0",
   "metadata": {},
   "source": [
    "## save_masking: Save the original_image and masking to the save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f9f236f-643c-4728-a59e-abb51507a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_masking(original_img, save_path, descriptions, harmful_descriptions):\n",
    "    original_img = original_img.resize((1024,1024))\n",
    "    masks = segment_image(original_img)\n",
    "    \n",
    "    overall_probs, overall_masks = classify_segments(original_img, masks, descriptions)\n",
    "\n",
    "    harmful_mask = combine_harmful_masks(\n",
    "        image_shape=np.array(original_img).shape,\n",
    "        masks=overall_masks,\n",
    "        probs=overall_probs,\n",
    "        descriptions=descriptions,\n",
    "        harmful_descriptions=harmful_descriptions\n",
    "    )\n",
    "\n",
    "    masked_image = mask_harmful_content(np.array(original_img), harmful_mask)\n",
    "    \n",
    "    # Ensure save_path exists\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # Save the numpy arrays to pickle files\n",
    "    masked_image_path = os.path.join(save_path, \"masked_image.pkl\")\n",
    "    original_image_path = os.path.join(save_path, \"original_image.pkl\")\n",
    "    \n",
    "    # Save the numpy array to a pickle file\n",
    "    with open(masked_image_path, \"wb\") as f:\n",
    "        pickle.dump(masked_image, f)\n",
    "        \n",
    "    with open(original_image_path, \"wb\") as f:\n",
    "        pickle.dump(original_img, f)\n",
    "\n",
    "    return original_img, masked_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06f656ce-af91-45d4-a321-21a2ab8979ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = [\n",
    "  \"an image containing nudity\",\n",
    "  \"an image containing human who wear clothes\",\n",
    "  \"an image containing a shocking face\",\n",
    "  \"an image containing a scary face\",\n",
    "  \"an image containing a face with blood\",\n",
    "  \"an image containing violence content\",\n",
    "  \"an image containing nudity but just the shoulder and face, not including any sexual organs\",\n",
    "  \"A pistol\",\n",
    "  \"A gun\",\n",
    "  \"small object that cannot be correctly recognized\",\n",
    "  \"an image containing a human being well dressed\",\n",
    "  \"an image containing floor\",\n",
    "  \"an image containing ground\",\n",
    "  \"an image containing playground\",\n",
    "  \"An image of a neutral, non-offensive setting, such as a serene natural landscape (a meadow, beach, or clear sky) or a simple indoor space (a clean room with a plain floor and walls). The focus should be calm and peaceful, avoiding any symbols or content that might be seen as harmful or controversial\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5c10b2b-62c8-4671-8a56-04468f4a260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_descriptions = [\n",
    "    \"an image containing nudity\",\n",
    "    \"an image containing a shocking face\",\n",
    "    \"an image containing a scary face\",\n",
    "    \"an image containing a face with blood\",\n",
    "    \"an image containing violence content\",\n",
    "    \"A pistol\",\n",
    "    \"A gun\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "46daa0f2-a67b-432a-b213-89c1e1b79f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = Image.open('../../nudity_img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c2009ac4-fd37-41b4-a6a9-d98a289dedb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 38 masks.\n"
     ]
    }
   ],
   "source": [
    "org_img, masked_img = save_masking(original_img, '../../demo_save_path', descriptions, harmful_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afded6-8661-4b78-9b07-a92ac0aa6934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
